# COMP8740-NEURAL_NETWORK_PROJECT
- **Weekly Plan**



- **Datasets**
-  Tiny-Imagnet-200

- **Hyper-Parameter Optimization**
	- Activation Function (ReLu, LeakyReLu- will tune the threshold here)
	- Generalized Loss Function (Categorical, sparse_categorical)
	- We'll try to define our Loss function (Later)
	- Learning Rate Scheduling
	- Optimizer (Adam, RMSProp, SGD)
	- Normalization (with different Data distribution) (Later)
	- Number of Hidden Layers (Need to optimize)
	- Number Units of each Hidden Layers(Need to find optimal hidden units)
	- Regularization (L2, Dropout)
	- Initializer (Xiver, He norma)
	- Data Augmentation



- **Create Hybrid model (Our Model + GAN)**


- **Define Tasks Step-by-step**
 -  
